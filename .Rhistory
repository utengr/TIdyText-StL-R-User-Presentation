library(data.table)
library("data.table", lib.loc="C:/Program Files/R/R-3.5.0/library")
library("data.table", lib.loc="C:/Program Files/R/R-3.5.0/library")
install.packages("data.table")
library("data.table", lib.loc="C:/Program Files/R/R-3.5.0/library")
detach("package:data.table", unload=TRUE)
shiny::runApp('D:/R_Stuff/Wanders/Wanders_Shiny')
install.packages(c("data.table", "httpuv", "later", "MASS", "stringi", "survival", "yaml"))
install.packages("survival")
install.packages("httpuv")
install.packages("stringi")
library(maps)
library(maps)
library(maptools)
library(rgdal)
eq = read.table(file="http://earthquake.usgs.gov/earthquakes/catalogs/eqs7day-M1.txt", fill=TRUE, sep=",", header=T)
plot.new()
my.map <- map("state", interior = FALSE, plot=F)
x.lim <- my.map$range[1:2]; x.lim[1] <- x.lim[1]-1; x.lim[2] <- x.lim[2]+1;
y.lim <- my.map$range[3:4]; y.lim[1] <- y.lim[1]-1; y.lim[2] <- y.lim[2]+1;
map("state", interior = FALSE, xlim=x.lim, ylim=y.lim)
map("state", boundary = FALSE, col="gray", add = TRUE)
title("Magnitude 1+ Earthquakes Over the Past 7 Days")
eq$mag.size <- NULL
eq$mag.size[eq$Magnitude>=1 & eq$Magnitude<2] <- .75
eq$mag.size[eq$Magnitude>=2 & eq$Magnitude<3] <- 1.0
eq$mag.size[eq$Magnitude>=3 & eq$Magnitude<4] <- 1.5
eq$mag.size[eq$Magnitude>=4] <- 2.0
eq$mag.col <- NULL
eq$mag.col[eq$Magnitude>=1 & eq$Magnitude<2] <- 'blue'
eq$mag.col[eq$Magnitude>=2 & eq$Magnitude<3] <- 'green'
eq$mag.col[eq$Magnitude>=3 & eq$Magnitude<4] <- 'orange'
eq$mag.col[eq$Magnitude>=4] <- 'red'
points(x=eq$Lon,y=eq$Lat,pch=16,cex=eq$mag.size, col=eq$mag.col)
eq$magnitude.text <- eq$Magnitude
eq$magnitude.text[eq$Magnitude<4] <- NA
text(x=eq$Lon,y=eq$Lat,col='black',labels=eq$magnitude.text,adj=c(2.5),cex=0.5)
legend('bottomright',c('M 1-2','M 2-3','M 3-4','M4+'), ncol=2,
pch=16, col=c('blue','green','orange','red'))
box()
install.packages("maptools")
install.packages("rgdal")
library(maps)
library(maptools)
library(rgdal)
eq = read.table(file="http://earthquake.usgs.gov/earthquakes/catalogs/eqs7day-M1.txt", fill=TRUE, sep=",", header=T)
plot.new()
my.map <- map("state", interior = FALSE, plot=F)
x.lim <- my.map$range[1:2]; x.lim[1] <- x.lim[1]-1; x.lim[2] <- x.lim[2]+1;
y.lim <- my.map$range[3:4]; y.lim[1] <- y.lim[1]-1; y.lim[2] <- y.lim[2]+1;
map("state", interior = FALSE, xlim=x.lim, ylim=y.lim)
map("state", boundary = FALSE, col="gray", add = TRUE)
title("Magnitude 1+ Earthquakes Over the Past 7 Days")
eq$mag.size <- NULL
eq$mag.size[eq$Magnitude>=1 & eq$Magnitude<2] <- .75
eq$mag.size[eq$Magnitude>=2 & eq$Magnitude<3] <- 1.0
eq$mag.size[eq$Magnitude>=3 & eq$Magnitude<4] <- 1.5
eq$mag.size[eq$Magnitude>=4] <- 2.0
eq$mag.col <- NULL
eq$mag.col[eq$Magnitude>=1 & eq$Magnitude<2] <- 'blue'
eq$mag.col[eq$Magnitude>=2 & eq$Magnitude<3] <- 'green'
eq$mag.col[eq$Magnitude>=3 & eq$Magnitude<4] <- 'orange'
eq$mag.col[eq$Magnitude>=4] <- 'red'
points(x=eq$Lon,y=eq$Lat,pch=16,cex=eq$mag.size, col=eq$mag.col)
eq$magnitude.text <- eq$Magnitude
eq$magnitude.text[eq$Magnitude<4] <- NA
text(x=eq$Lon,y=eq$Lat,col='black',labels=eq$magnitude.text,adj=c(2.5),cex=0.5)
legend('bottomright',c('M 1-2','M 2-3','M 3-4','M4+'), ncol=2,
pch=16, col=c('blue','green','orange','red'))
box()
gas <- ts(read.csv("https://robjhyndman.com/data/gasoline.csv", header=FALSE)[,1],
freq=365.25/7, start=1991+31/365.25)
gas
test <- read.csv("https://robjhyndman.com/data/gasoline.csv")
test
?ts
test[,1]
library(forecast)
?tbats
test <- read.csv("https://robjhyndman.com/data/gasoline.csv")
test
gas <- ts(read.csv("https://robjhyndman.com/data/gasoline.csv", header=FALSE)[,1],
freq=365.25/7, start=1991+31/365.25)
gastbats <- tbats(gas)
fc2 <- forecast(gastbats, h=104)
plot(fc2, ylab="thousands of barrels per day")
fc2
str(fcs)
str(fc2)
fc2
head(fc2)
head(fc2, 10)
fc2
str(fc2)
?plot
fc2$x
test <- fc2
test
str(test)
fc2$mean
test <- fc2$mean
test <- as.data.frame(fc2$mean)
test
str(fc2)
test2 <- fc2$x
test2
plot(forecast(gastbats, h=104))
plot(forecast(gastbats, h=104), include = X)
plot(forecast(gastbats, h=104), include = x)
plot(forecast(gastbats, h=104), include = X)
?plot.forecast
plot(forecast(gastbats, h=104), include = 0)
plot(forecast(gastbats, h=104), include = 50)
plot(forecast(gastbats, h=104), include = 150)
plot(forecast(gastbats, h=104), include = 100)
plot(fc2, ylab="thousands of barrels per day", include = 100)
plot(fc2, ylab="thousands of barrels per day")
plot(fc2, ylab="thousands of barrels per day", include = 100)
test
test
str(test)
test <- read.csv("https://robjhyndman.com/data/gasoline.csv")
test
str(test)
summary(test)
plot(test)
names
names(test)
test <- mnist
test <- "iris"
test
test <- iris
test
names(test'')
names(test)
plot(Sepal.length, Sepal.Eidth, test)
plot(Sepal.length, Sepal.Width, test)
plot(Sepal.Length, Sepal.Width, test)
plot(Sepal.Lentgh, Sepal.Width, test)
plot(Sepal.Length, Sepal.Width, test)
plot(test$Sepal.Length, test$Sepal.Width)
plot(test$Sepal.Length, test$Sepal.Width, col = "red")
plot(test$Sepal.Length, test$Sepal.Width, col = "red", pch =19)
plot(test$Sepal.Length, test$Sepal.Width, col = test$Species, pch =19)
setwd("D:/UseR")
library(tidyverse)
library(tidytext)
setwd("~/")
library('tidyverse')
library('rvest')
base_url = 'https://www.indeed.com/jobs?as_and=human+resources&as_phr=&as_any=&as_not=&as_ttl=human+resources&as_cmp=&jt=fulltime&st=&salary=&radius=25&l=63116&fromage=any&limit=10&sort=date&psf=advsrch'
num_pages = 10
current_url = base_url
titles = tibble()
summaries = tibble()
for (i in 1:num_pages) {
current_url_page = read_html(current_url)
ads = current_url_page %>%
html_nodes('[class="  row  result"]')
for (j in 1:length(ads)) {
titles = ads[[j]] %>%
html_nodes('[class=jobtitle]') %>%
html_text() %>%
as.tibble() %>%
rbind(titles)
summaries = ads[[j]] %>%
html_nodes('[class=summary]') %>%
html_text() %>%
as.tibble() %>%
rbind(summaries)
}
current_url = paste0(base_url, '&start=', as.character(i * 10))
}
summaries <- gsub("/n", "", summaries)
summaries <- gsub("  ", "", summaries)
summaries <- gsub("\n", "", summaries)
base_url = 'https://www.indeed.com/jobs?as_and=human+resources&as_phr=&as_any=&as_not=&as_ttl=human+resources&as_cmp=&jt=fulltime&st=&salary=&radius=25&l=63116&fromage=any&limit=10&sort=date&psf=advsrch'
num_pages = 10
current_url = base_url
titles = tibble()
summaries = tibble()
for (i in 1:num_pages) {
current_url_page = read_html(current_url)
ads = current_url_page %>%
html_nodes('[class="  row  result"]')
for (j in 1:length(ads)) {
titles = ads[[j]] %>%
html_nodes('[class=jobtitle]') %>%
html_text() %>%
as.tibble() %>%
rbind(titles)
summaries = ads[[j]] %>%
html_nodes('[class=summary]') %>%
html_text() %>%
as.tibble() %>%
rbind(summaries)
}
current_url = paste0(base_url, '&start=', as.character(i * 10))
}
summaries <- gsub("\n", "", summaries)
summaries <- gsub("  ", "", summaries)
summaries
summaries[1,]
summaries[1]
summaries(1,)
titles = tibble()
summaries = tibble()
for (i in 1:num_pages) {
current_url_page = read_html(current_url)
ads = current_url_page %>%
html_nodes('[class="  row  result"]')
for (j in 1:length(ads)) {
titles = ads[[j]] %>%
html_nodes('[class=jobtitle]') %>%
html_text() %>%
as.tibble() %>%
rbind(titles)
summaries = ads[[j]] %>%
html_nodes('[class=summary]') %>%
html_text() %>%
as.tibble() %>%
rbind(summaries)
}
current_url = paste0(base_url, '&start=', as.character(i * 10))
}
summaries
?gsub
str(summaries)
summaries <- gsub("\n", "", summaries)
str(summaries)
summaries
titles = tibble()
summaries = tibble()
for (i in 1:num_pages) {
current_url_page = read_html(current_url)
ads = current_url_page %>%
html_nodes('[class="  row  result"]')
for (j in 1:length(ads)) {
titles = ads[[j]] %>%
html_nodes('[class=jobtitle]') %>%
html_text() %>%
as.tibble() %>%
rbind(titles)
summaries = ads[[j]] %>%
html_nodes('[class=summary]') %>%
html_text() %>%
as.tibble() %>%
rbind(summaries)
}
current_url = paste0(base_url, '&start=', as.character(i * 10))
}
summaries <- as.data.frame(summaries)
summaries
summaries <- gsub("\n", "", summaries)
titles = tibble()
summaries = tibble()
for (i in 1:num_pages) {
current_url_page = read_html(current_url)
ads = current_url_page %>%
html_nodes('[class="  row  result"]')
for (j in 1:length(ads)) {
titles = ads[[j]] %>%
html_nodes('[class=jobtitle]') %>%
html_text() %>%
as.tibble() %>%
rbind(titles)
summaries = ads[[j]] %>%
html_nodes('[class=summary]') %>%
html_text() %>%
as.tibble() %>%
rbind(summaries)
}
current_url = paste0(base_url, '&start=', as.character(i * 10))
}
summaries <- as.data.frame(summaries, stringsAsFactors = FALSE )
summaries
str(summaries)
summaries <- gsub("\n", "", summaries)
str(summaries)
titles = tibble()
summaries = tibble()
for (i in 1:num_pages) {
current_url_page = read_html(current_url)
ads = current_url_page %>%
html_nodes('[class="  row  result"]')
for (j in 1:length(ads)) {
titles = ads[[j]] %>%
html_nodes('[class=jobtitle]') %>%
html_text() %>%
as.tibble() %>%
rbind(titles)
summaries = ads[[j]] %>%
html_nodes('[class=summary]') %>%
html_text() %>%
as.tibble() %>%
rbind(summaries)
}
current_url = paste0(base_url, '&start=', as.character(i * 10))
}
summaries <- gsub("\\n", "", summaries)
titles = tibble()
summaries = tibble()
for (i in 1:num_pages) {
current_url_page = read_html(current_url)
ads = current_url_page %>%
html_nodes('[class="  row  result"]')
for (j in 1:length(ads)) {
titles = ads[[j]] %>%
html_nodes('[class=jobtitle]') %>%
html_text() %>%
as.tibble() %>%
rbind(titles)
summaries = ads[[j]] %>%
html_nodes('[class=summary]') %>%
html_text() %>%
as.tibble() %>%
rbind(summaries)
}
current_url = paste0(base_url, '&start=', as.character(i * 10))
}
summaries <- as.data.frame(summaries, stringsAsFactors = FALSE)
temp <- gsub("\\n", "", summaries)
temp
summaries
temp <- gsub("\n", "", summaries)
temp <- gsub(\n, "", summaries)
temp <- gsub("\\\n", "", summaries)
temp <- gsub("\\n", "", summaries)
str(temp)
temp <- gsub("\\n", "", summaries(1.))
temp <- gsub("\\n", "", summaries(1,))
temp <- gsub("\\n", "", summaries[1,]
)
temp
temp <- gsub("\\n", "", summaries[,1]
)
summaries <- gsub("\\n", "", summaries[,1])
summaries <- gsub("  ", "", summaries)
rm(temp)
names(summaries)
summaries
?colnames
?colnames
?colNames
names(summaries) <- "summary"
str(summaries)
titles
summary(summaries)
text_df <- data_frame(line = 1:90, text = summaries)
text_df
titles = tibble()
summaries = tibble()
for (i in 1:num_pages) {
current_url_page = read_html(current_url)
ads = current_url_page %>%
html_nodes('[class="  row  result"]')
for (j in 1:length(ads)) {
titles = ads[[j]] %>%
html_nodes('[class=jobtitle]') %>%
html_text() %>%
as.tibble() %>%
rbind(titles)
summaries = ads[[j]] %>%
html_nodes('[class=summary]') %>%
html_text() %>%
as.tibble() %>%
rbind(summaries)
}
current_url = paste0(base_url, '&start=', as.character(i * 10))
}
base_url = 'https://www.indeed.com/jobs?as_and=human+resources&as_phr=&as_any=&as_not=&as_ttl=human+resources&as_cmp=&jt=fulltime&st=&salary=&radius=25&l=63116&fromage=any&limit=10&sort=date&psf=advsrch'
num_pages = 10
current_url = base_url
titles = tibble()
summaries = tibble()
for (i in 1:num_pages) {
current_url_page = read_html(current_url)
ads = current_url_page %>%
html_nodes('[class="  row  result"]')
for (j in 1:length(ads)) {
titles = ads[[j]] %>%
html_nodes('[class=jobtitle]') %>%
html_text() %>%
as.tibble() %>%
rbind(titles)
summaries = ads[[j]] %>%
html_nodes('[class=summary]') %>%
html_text() %>%
as.tibble() %>%
rbind(summaries)
}
current_url = paste0(base_url, '&start=', as.character(i * 10))
}
summaries <- data_frame(text = summaries, stringsAsFactors = FALSE)
summaries <- data_frame(text = summaries[,1], stringsAsFactors = FALSE)
?data_frame
summaries <- as.data.frame(summaries, stringsAsFactors = FALSE)
summaries <- gsub("\\n", "", summaries[,1])
summaries <- gsub("  ", "", summaries)
text_df <- data_frame(text = summaries)
text_df <- unnest_tokens(text_df, word, text)
library(tidytext)
text_df <- unnest_tokens(text_df, word, text)
text_df
text_df %>% count(word, sort = TRUE)
count(stop_words, lexicon)
?anti_join
library('tidyverse')
library('rvest')
#Analysis
library(tidytext)
?anti_join
count(stop_words, lexicon)
names(stop_words
)
base_url = 'https://www.indeed.com/jobs?as_and=human+resources&as_phr=&as_any=&as_not=&as_ttl=human+resources&as_cmp=&jt=fulltime&st=&salary=&radius=25&l=63116&fromage=any&limit=10&sort=date&psf=advsrch'
num_pages = 10
current_url = base_url
titles = tibble()
summaries = tibble()
for (i in 1:num_pages) {
current_url_page = read_html(current_url)
ads = current_url_page %>%
html_nodes('[class="  row  result"]')
for (j in 1:length(ads)) {
titles = ads[[j]] %>%
html_nodes('[class=jobtitle]') %>%
html_text() %>%
as.tibble() %>%
rbind(titles)
summaries = ads[[j]] %>%
html_nodes('[class=summary]') %>%
html_text() %>%
as.tibble() %>%
rbind(summaries)
}
current_url = paste0(base_url, '&start=', as.character(i * 10))
}
#Analysis
library(tidytext)
summaries <- as.data.frame(summaries, stringsAsFactors = FALSE)
summaries <- gsub("\\n", "", summaries[,1])
summaries <- gsub("  ", "", summaries)
text_df <- data_frame(text = summaries)
text_df <- unnest_tokens(text_df, word, text)
text_df <- anti_join(text_df, stop_words, by = word)
text_df <- anti_join(text_df, stop_words)
text_df %>% count(word, sort = TRUE)
tail(text_df)
head(text_df, 40)
text_df %>% count(word, sort = TRUE)
text_df <- text_df %>% bind_tf_idf(word, n)
text_df <- text_df %>% count(word, sort = TRUE)
text_df
text_df <- text_df %>% count(word, n, sort = TRUE)
text_df <- text_df %>% count(word, sort = TRUE)
text_df <- text_df %>% bind_tf_idf(word, n)
?bind_tf_idf
?bind_tf_idf
?bind_tf_idf
text_df <- as.data.frame(text_df, stringsAsFactors = FALSE)
text_df
text_df <- text_df %>% count(word, sort = TRUE)
text_df
text_df <- data_frame(text = summaries)
text_df <- unnest_tokens(text_df, word, text)
text_df <- anti_join(text_df, stop_words)
text_df <- text_df %>% count(word, sort = TRUE)
text_df
options(tibble.print_max = Inf)
text_df
?unnest_tokens
text_df <- unnest_tokens(text_df, ngram, text, token = "ngrams", n = 2)
text_df <- unnest_tokens(text_df, ngram, word, token = "ngrams", n = 2)
text_df <- anti_join(text_df, stop_words)
text_df <- data_frame(text = summaries)
text_df <- anti_join(text_df, stop_words)
text_df <- data_frame(text = summaries)
text_df
text_df <- anti_join(text_df, stop_words, by = text)
text_df <- unnest_tokens(text_df, ngram, word, token = "ngrams", n = 2)
text_df <- data_frame(text = summaries)
text_df <- unnest_tokens(text_df, ngram, word, token = "ngrams", n = 2)
text_df <- text_df %>% count(word, sort = TRUE)
text_df
text_df <- data_frame(text = summaries)
text_df
text_df <- unnest_tokens(text_df, word, text)
text_df <- anti_join(text_df, stop_words)
text_df <- data_frame(text = summaries)
text_df <- unnest_tokens(text_df, ngram, text, token = "ngrams", n = 2)
text_df
text_df <- text_df %>% count(word, sort = TRUE)
text_df <- text_df %>% count(ngram, sort = TRUE)
text_df
head(text_df)
head(text_df, 20)
text_df <- data_frame(text = summaries)
text_df <- unnest_tokens(text_df, word, text)
# text_df <- unnest_tokens(text_df, ngram, text, token = "ngrams", n = 2)
text_df <- anti_join(text_df, stop_words)
text_df <- text_df %>% count(text(), sort = TRUE)
# text_df <- unnest_tokens(text_df, ngram, text, token = "ngrams", n = 2)
text_df <- anti_join(text_df, stop_words)
text_df <- text_df %>% count(text, sort = TRUE)
text_df <- text_df %>% count(word, sort = TRUE)
text_df
summaries[1,]
summaries[,1]
summaries
summaries[1]
titles
