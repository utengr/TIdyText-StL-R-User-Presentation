`r if(file.exists("head.R")) { source("head.R"); I(head) }`
`r if(file.exists("debug.R")) { source("debug.R"); I(grid) }`

Using TidyText to Streamline NLP Workflows
===
author: Dennis Chandler
date: `r format(Sys.time(), '%d %B, %Y')`
width: 1500
height: 800
transition: linear
transition-speed: fast
css: rpres.css
class: larger
<!-- NOTE: Styling and external images may be missing --> 

<p>R Enthusiast
  <br/>
  St. Louis, MO
  <br/>
</p>

<img src="img/R.png" />

What is "Text Mining with R"?
===
class: larger center-img

* Package for "Tidy" Analysis of Text
* Ability to work with Various Text Analysis Formats

<img src="img/bookcover.png" height="500px" />

Tidy Text Format
===
class: larger
* Asimov Quotes

```{r, echo = FALSE}
text <- c("Violence is the last refuge of the incompetent",
"The saddest aspect of life right now is that science gathers knowledge faster than society gathers wisdom", "People who think they know everything are a great annoyance to those of us who do", "The most exciting phrase to hear in science, the one that heralds new discoveries, is not 'Eureka!' (I've found it!), but 'That's funny...'")
text
```

Tidy Text Format
===

```{r}
library(tidytext); library(tidyverse)
text_df <- data_frame(line = 1:4, text = text)
text_df <- unnest_tokens(text_df, word, text)
text_df
```

Format Advantages
===
```{r}
text_df %>% count(word, sort = TRUE)
```

N-Grams!!
===

```{r}
text_df <- data_frame(line = 1:4, text = text)
text_df <- unnest_tokens(text_df, ngram, text, token = "ngrams", n =3)
text_df
```

Additional Items
===
class: larger
* Built in stop_words
  * onix, SMART, & snowball
  * http://www.lextek.com/manuals/onix/stopwords1.html
  * http://www.jmlr.org/papers/volume5/lewis04a/lewis04a.pdf
  * http://snowball.tartarus.org/algorithms/english/stop.txt

* Use dplyr's anti_join or use other techniques
```{r, echo = FALSE}
count(stop_words, lexicon)
```

Text Analysis
===
class: smaller
* Novel Example, The War of the Worlds by H.G. Wells

```{r}
library(gutenbergr); library(stringr)
novel <- gutenberg_download(36)
chapter <- mutate(novel, linenumber = row_number(),
        chapter = cumsum(str_detect(text,
        regex("^chapter ", ignore_case = TRUE))))
tail(chapter)
```

All Columns Preserved
===
class: smaller
```{r}
token_novel <- unnest_tokens(chapter, word, text)
tail(token_novel, 15)
```

Sentiment
===
* Built in sentiment lexicons
  * AFINN, bing, nrc, and loughran
  * http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010
  * https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html
  * http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm
  * http://www3.nd.edu/~mcdonald/Word_Lists.html

* Use dplyr's inner_join or use other techniques
```{r, echo = FALSE}
count(sentiments, lexicon)
```

Putting it all Together
===
class: graph-img smaller
* Look over 80 lines
```{r, results = FALSE, fig.height = 2, fig.width = 5, fig.align = "center"}
WOTW_sentiment <- token_novel %>% inner_join(get_sentiments("bing")) %>%
  count(index = linenumber %/% 80, sentiment) %>% spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

ggplot(WOTW_sentiment, aes(index, sentiment, fill = 'red')) +
         geom_col(show.legend = FALSE)
```

TF-IDF
===
class: graph-img smaller
* TF-IDF per Chapter
```{r}
token_novel <- unnest_tokens(chapter, word, text) %>%
               count(chapter, word, sort = TRUE) %>% ungroup()

chapter_novel <- bind_tf_idf(token_novel, word, chapter, n)
chapter_novel <- arrange(chapter_novel, desc(tf_idf))
chapter_novel[10:20,]
```

Other Text Data Formats
===
class: larger
* Commonly used schema is Document Term Matrix (DTM)
* Also Document-Feature Matrix (DFM)
* Used in tm, topicmodels, Quanteda, and several other packages
* DTMs and DFMs are usually in a Bag-of-Words structure
<br/>
<br/>
tidy() converts to a 'tidy' dataset
  * Will remove zero occuring words
  * Will create columns for document, word, and count
  * Will preserve Metadata

Coverting to other formats
===
class: larger
* cast_tdm( ) -> Term Document Matrix (tm package)
* cast_dtm( ) -> Document Term Matrix (tm package)
* cast_dfm( ) -> Document-Feature Matrix (quanteda package)


* Can just cast_sparse( ) for generic sparse matrix

Workflow
===
class: center-img

<img src="img/workflow.png" height="750px" />

Initial Motivation: word2vec
===
class: larger center-img
* Deep Learning to Understand Language
* StitchFix Blog on Alternative Method
  * Chris Moody; Multithreaded - October 18th, 2017
* Information Theory of Pointwise Mutual Information


<img src="img/equation.gif" height="300px" />

Let's Implement!
===
```{r, eval = FALSE}
library(bigrquery)
library(tidyverse)

project <- "my-first-project-184914"

sql <- "#legacySQL
 SELECT
   stories.title AS title,
   stories.text AS text
 FROM
   [bigquery-public-data:hacker_news.full] AS stories
 WHERE
   stories.deleted IS NULL
 LIMIT
   250000"

hacker_news_raw <- query_exec(sql, project = project, max_pages = Inf)
```

Let's find p(x) and p(y)
===
class: smaller

```{r, cache = TRUE, echo = FALSE}
hacker_news_text <- read.csv("hacker_news.csv", stringsAsFactors = FALSE)
library(tidytext)
```
```{r, cache = TRUE}
unigram_probs <- hacker_news_text %>%
  unnest_tokens(word, text) %>%
  count(word, sort = TRUE) %>%
  mutate(p = n / sum(n))

unigram_probs
```

Now p(x,y)
===
class: center-img
```{r, eval = FALSE, cache = TRUE}
library(widyr)

tidy_skipgrams <- hacker_news_text %>%
  unnest_tokens(ngram, text, token = "ngrams", n = 8) %>%
  mutate(ngramID = row_number()) %>% 
  unite(skipgramID, postID, ngramID) %>%
  unnest_tokens(word, ngram)

skipgram_probs <- tidy_skipgrams %>%
  pairwise_count(word, skipgramID, diag = TRUE, sort = TRUE) %>%
  mutate(p = n / sum(n))
```

Now p(x,y)
===
class: center-img
```{r, eval = FALSE, cache = TRUE}
library(widyr)

tidy_skipgrams <- hacker_news_text %>%
  unnest_tokens(ngram, text, token = "ngrams", n = 8) %>%
  mutate(ngramID = row_number()) %>% 
  unite(skipgramID, postID, ngramID) %>%
  unnest_tokens(word, ngram)

skipgram_probs <- tidy_skipgrams %>%
  pairwise_count(word, skipgramID, diag = TRUE, sort = TRUE) %>%
  mutate(p = n / sum(n))
```
<img src="img/skip_grams.gif" height="200px" />

Now p(x,y)
===
class: center-img
```{r, eval = FALSE, cache = TRUE}
library(widyr)

tidy_skipgrams <- hacker_news_text %>%
  unnest_tokens(ngram, text, token = "ngrams", n = 8) %>%
  mutate(ngramID = row_number()) %>% 
  unite(skipgramID, postID, ngramID) %>%
  unnest_tokens(word, ngram)

skipgram_probs <- tidy_skipgrams %>%
  pairwise_count(word, skipgramID, diag = TRUE, sort = TRUE) %>%
  mutate(p = n / sum(n))
```
<img src="img/grams_size.gif" height="300px" />


p(x,y) continued
===
class: center-img
```{r, eval = FALSE, cache = TRUE}
normalized_prob <- skipgram_probs %>%
  filter(n > 20) %>%
  rename(word1 = item1, word2 = item2) %>%
  left_join(unigram_probs %>%
              select(word1 = word, p1 = p), by = "word1") %>%
  left_join(unigram_probs %>%
              select(word2 = word, p2 = p), by = "word2") %>%
  mutate(p_together = p / p1 / p2
```

<img src="img/equation.gif" height="200px" />


PMI Matrix
===
class: center-img
```{r, eval = FALSE, cache = TRUE}
pmi_matrix <- normalized_prob %>%
  mutate(pmi = log10(p_together)) %>%
  cast_sparse(word1, word2, pmi)
```
<img src="img/PMI.jpg" height="500px" />

Dimension Reduction into Word Vectors
===
* The augmented implicitly restarted Lanczos bidiagonalization algorithm (IRLBA) finds a few ap-
proximate largest (or, optionally, smallest) singular values and corresponding singular vectors of a
sparse or dense matrix using a method of Baglama and Reichel.  It is a fast and memory-efficient
way to compute a partial SVD.
```{r, eval = FALSE, cache = TRUE}
library(irlba)

pmi_svd <- irlba(pmi_matrix, 256, maxit = 1e3)
```

Dimension Reduction into Word Vectors
===
class: center-img
* The augmented implicitly restarted Lanczos bidiagonalization algorithm (IRLBA) finds a few ap-
proximate largest (or, optionally, smallest) singular values and corresponding singular vectors of a
sparse or dense matrix using a method of Baglama and Reichel.  It is a fast and memory-efficient
way to compute a partial SVD.
```{r, eval = FALSE, cache = TRUE}
library(irlba)

pmi_svd <- irlba(pmi_matrix, 256, maxit = 1e3)
```
<br/>
<img src="img/irlba_error.gif" height="200px" />

When all else fails...
===
class: center-img
<img src="img/package_update.gif" height="750px" />

But why NA's?
===

But why NA's?
===
class:center-img
<img src="img/NAs.gif" height="750px" />

Dimension Reduction into Word Vectors
===
class: smaller
```{r, eval = FALSE, cache = TRUE}
library(irlba)

pmi_svd <- irlba(pmi_matrix, 256, maxit = 1e3)

word_vectors <- pmi_svd$u
rownames(word_vectors) <- rownames(pmi_matrix)
```
```{r, echo = FALSE, cache = TRUE}
word_vectors = read.csv('word_vectors.csv', row.names = 1)
word_vectors <- as.matrix(word_vectors)
head(word_vectors[,1:5], 10)
```

Word Vectors... Now what?
===
* 49,422 words as a 256 size vector
<br/>
```{r}
library(broom)

search_synonyms <- function(word_vectors, selected_vector) {
  
  similarities <- word_vectors %*% selected_vector %>%
    tidy() %>%
    rename(token = .rownames,
           similarity = unrowname.x.)
  
  similarities %>%
    arrange(-similarity)    
}
```

Let's see what we have...
===

Let's see what we have...
===
class: smaller
```{r}
facebook <- search_synonyms(word_vectors, word_vectors["facebook",])
head(facebook, 10)
```

Let's see what we have...
===
class: smaller
```{r}
haskell <- search_synonyms(word_vectors, word_vectors["haskell",])
head(haskell, 10)
```

Word Math
===
<img src="img/word2vec-king-queen-composition.png" height="750px" />

Word Math
===
class: smaller
```{r}
mystery_product <- word_vectors["iphone",] - word_vectors["apple",] + word_vectors["microsoft",]
head(search_synonyms(word_vectors, mystery_product), 20)
```

Word Math
===
class: smaller
```{r}
mystery_product <- word_vectors["iphone",] - word_vectors["apple",] + word_vectors["amazon",]
head(search_synonyms(word_vectors, mystery_product),20)
```

Beyond Word Vectors
===
class: smaller
Dynamic Word Embeddings for Evolving Semantic Discovery <br/>
Zijun Yao, Yifan Sun, Weicong Ding, Nikhil Rao, Hui Xiong <br/>
(Submitted on 2 Mar 2017 (v1), last revised 13 Feb 2018 (this version, v2))

    Word evolution refers to the changing meanings and associations of words throughout time, as a byproduct of human language evolution. By studying word evolution, we can infer social trends and language constructs over different periods of human history. However, traditional techniques such as word representation learning do not adequately capture the evolving language structure and vocabulary. In this paper, we develop a dynamic statistical model to learn time-aware word vector representation. We propose a model that simultaneously learns time-aware embeddings and solves the resulting "alignment problem". This model is trained on a crawled NYTimes dataset. Additionally, we develop multiple intuitive evaluation strategies of temporal word embeddings. Our qualitative and quantitative tests indicate that our method not only reliably captures this evolution over time, but also consistently outperforms state-of-the-art temporal embedding approaches on both semantic accuracy and alignment quality. 

Comments: 	9 pages, published in the International Conference on Web Search and Data Mining (WSDM 2018) <br/>
Subjects: 	Computation and Language (cs.CL); Machine Learning (stat.ML) <br/>
DOI: 	10.1145/3159652.3159703 <br/>
Cite as: 	arXiv:1703.00607 [cs.CL] <br/>

https://arxiv.org/abs/1703.00607

Beyond Word Vectors
===
class: center-img
<img src="img/temporal_embedding.jpeg" height="700px" width= "1860px" />

Can do the same with TidyText
===
<br/>
<br/>
<img src="img/multi_tensor.gif" height="400px" />

Can do the same with TidyText
===
<br/>
<br/>
<img src="img/multi_tensor.gif" height="400px" />
<br/>
hosvd() from rTensor Package??

===
class: larger-500 center-txt
<br/>
Thank You

